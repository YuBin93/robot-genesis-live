# api/deep_analyze.py

from http.server import BaseHTTPRequestHandler
import json
import os
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- 1. é…ç½®ä¸åˆå§‹åŒ– (ä¸analyze.pyç±»ä¼¼) ---
API_KEY = os.getenv("GEMINI_API_KEY")
if API_KEY:
    genai.configure(api_key=API_KEY)
else:
    print("CRITICAL WARNING: GEMINI_API_KEY environment variable not found.")

# --- 2. ç½‘é¡µæŠ“å–è¾…åŠ©å‡½æ•° ---
def scrape_url(url):
    """æŠ“å–å•ä¸ªURLçš„æ–‡æœ¬å†…å®¹"""
    try:
        print(f"  - Scraping: {url}")
        response = requests.get(url, headers={'User-Agent': 'Robot-Genesis-Deep-Dive/1.0'}, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        # æå–æ›´å¹²å‡€çš„æ­£æ–‡
        body = soup.find('body')
        if body:
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼ï¼Œå‡å°‘å™ªéŸ³
            for element in body(['script', 'style']):
                element.decompose()
            return body.get_text(separator=' ', strip=True)
        return ""
    except Exception as e:
        print(f"  - Failed to scrape {url}: {e}")
        return ""

# --- 3. æ·±åº¦åˆ†æPromptæ„å»ºå‡½æ•° ---
def build_deep_analysis_prompt(comprehensive_text):
    """æ„å»ºä¸€ä¸ªç”¨äºæ·±åº¦æŠ€æœ¯åˆ†æçš„Prompt"""
    desired_json_structure = """
    {
      "technical_summary": "A summary of the robot's key technical specifications and innovations based on all provided text.",
      "perception_system": {
        "components": ["List of all mentioned sensor types like 'RGB cameras', 'LiDAR', 'IMU', 'microphones'."],
        "suppliers_and_partners": ["List of companies involved, e.g., 'NVIDIA', 'OpenAI'."],
        "analysis": "A brief analysis of the perception system's capabilities."
      },
      "locomotion_system": {
        "components": ["List of mechanical parts like 'Actuators', 'degrees of freedom (DOF)', 'battery system'."],
        "suppliers_and_partners": ["List any mentioned hardware partners."],
        "analysis": "A brief analysis of the locomotion system's design and performance."
      },
      "control_and_ai_system": {
        "components": ["Mention of 'AI models', 'neural networks', 'computational hardware'."],
        "suppliers_and_partners": ["List key AI partners like 'OpenAI', 'Google'."],
        "analysis": "A brief analysis of the robot's AI brain and control strategy."
      }
    }
    """
    prompt = f"""
    You are a senior robotics engineer. Analyze the comprehensive text compiled from multiple sources about a specific robot.
    Your task is to perform a deep technical dive and structure your findings into a valid JSON object.
    Focus on identifying and analyzing the robot's core systems. Be detailed and specific.

    ### Desired JSON Structure:
    {desired_json_structure}

    ---
    ### Comprehensive Text from Multiple Sources:
    {comprehensive_text[:30000]}
    ---

    ### Deep Technical Analysis (JSON Output):
    """
    return prompt

# --- 4. æ™ºèƒ½JSONæå–å‡½æ•° (ä¿æŒä¸å˜) ---
def extract_json_from_text(text):
    # ... (ä¸ä¸Šä¸€ç‰ˆå®Œå…¨ç›¸åŒï¼Œä¸ºäº†ç®€æ´çœç•¥) ...
    match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', text, re.DOTALL)
    if match:
        try: return json.loads(match.group(1))
        except json.JSONDecodeError: pass
    try:
        start_index = text.find('{')
        end_index = text.rfind('}')
        if start_index != -1 and end_index != -1 and end_index > start_index:
            return json.loads(text[start_index : end_index + 1])
    except json.JSONDecodeError: pass
    raise json.JSONDecodeError("Could not find a valid JSON object in the model's response.", text, 0)

# --- 5. Serverless Function ä¸»å¤„ç†é€»è¾‘ ---
class handler(BaseHTTPRequestHandler):
    def do_POST(self):
        if not API_KEY:
            # ... (é”™è¯¯å¤„ç†) ...
            return
        
        try:
            # æ­¥éª¤A: è·å–å‰ç«¯å‘é€è¿‡æ¥çš„URLåˆ—è¡¨
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            body = json.loads(post_data)
            urls = body.get('urls', [])

            if not urls:
                raise ValueError("No URLs provided for deep analysis.")
            
            # æ­¥éª¤B: å¹¶å‘æŠ“å–æ‰€æœ‰URLçš„å†…å®¹
            print(f"ğŸš€ Starting deep dive analysis on {len(urls)} sources...")
            all_texts = []
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æŠ“å–ï¼Œæé«˜æ•ˆç‡
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {executor.submit(scrape_url, url): url for url in urls}
                for future in as_completed(future_to_url):
                    all_texts.append(future.result())
            
            comprehensive_text = "\n\n--- NEW SOURCE ---\n\n".join(filter(None, all_texts))
            
            if not comprehensive_text.strip():
                raise ValueError("Could not scrape any content from the provided URLs.")

            # æ­¥éª¤C: AIè¿›è¡Œæ·±åº¦åˆ†æ
            print("ğŸ§  Performing deep analysis with Gemini...")
            prompt = build_deep_analysis_prompt(comprehensive_text)
            model = genai.GenerativeModel('gemini-1.5-flash-latest')
            safety_settings = [ {"category": c, "threshold": "BLOCK_NONE"} for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"] ]
            gemini_response = model.generate_content(prompt, safety_settings=safety_settings)
            
            deep_analysis_data = extract_json_from_text(gemini_response.text)

            # æ­¥éª¤D: æˆåŠŸè¿”å›æ·±åº¦åˆ†æç»“æœ
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps(deep_analysis_data).encode())

        except Exception as e:
            # ... (é”™è¯¯å¤„ç†) ...
            print(f"âŒ An error occurred during deep analysis: {e}")
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({"error": f"An internal error occurred: {str(e)}"}).encode())
